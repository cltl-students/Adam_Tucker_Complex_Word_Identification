{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5b6b0a-8df3-4429-9ff4-7c85fa60e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4addbb-f7c0-42bf-997b-3531db160b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the classifier to without the raw Ngram data\n",
    "\n",
    "\n",
    "# 1- Classifier should now handle NaN values and change them to zero\n",
    "\n",
    "def train_classifier(word_features_file):\n",
    "    # Load the word_features DataFrame from the pickle file\n",
    "    with open(word_features_file, 'rb') as file:\n",
    "        word_features = pickle.load(file)\n",
    "\n",
    "    # Replace empty values with NaN\n",
    "    word_features.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    word_features.fillna(0, inplace=True)\n",
    "\n",
    "    # Extract the features and labels from the word_features DataFrame\n",
    "    features_columns = ['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount','relTotalMatchCount'] \n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "    # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(classifier, output_folder, word_features_file):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the base filename from the word_features_file\n",
    "    base_filename = os.path.splitext(os.path.basename(word_features_file))[0]\n",
    "\n",
    "    # Remove \"Feats\" from the base filename, if present\n",
    "    base_filename = base_filename.replace(\"Feats\", \"\")\n",
    "\n",
    "    # Save the classifier model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{base_filename}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "\n",
    "# Train the classifier\n",
    "word_features_file = 'features_NEW/Wikipedia_Train_NEW_Feats1.pkl'\n",
    "classifier = train_classifier(word_features_file)\n",
    "\n",
    "# Save the classifier model to the \"lmodel\" folder\n",
    "output_folder = 'lmodel'\n",
    "save_model(classifier, output_folder, word_features_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00228d88-c521-456c-b3b3-a1d2a746651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New code for combined data do not change\n",
    "\n",
    "def train_classifier(word_features_file1, word_features_file2):\n",
    "    # Load the first word_features DataFrame from the pickle file\n",
    "    with open(word_features_file1, 'rb') as file:\n",
    "        word_features1 = pickle.load(file)\n",
    "\n",
    "    # Load the second word_features DataFrame from the pickle file\n",
    "    with open(word_features_file2, 'rb') as file:\n",
    "        word_features2 = pickle.load(file)\n",
    "\n",
    "    # Concatenate the two word_features DataFrames\n",
    "    word_features = pd.concat([word_features1, word_features2], ignore_index=True)\n",
    "\n",
    "    # Extract the features and labels from the combined word_features DataFrame\n",
    "    features_columns = ['syllables', 'length', 'vowels', 'simple_wiki_freq', 'HIT_freq', 'google frequency']\n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "    # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(classifier, output_folder, model_name):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save the classifier model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{model_name}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3ee509-670b-4174-888d-05546e8ddb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_features_file1 = 'features/Wikipedia_Dev_allInfo_1.pkl'\n",
    "word_features_file2 = 'features/Wikipedia_Train_allInfo_1.pkl'\n",
    "classifier = train_classifier(word_features_file1, word_features_file2)\n",
    "\n",
    "\n",
    "# Save the classifier model to the \"lm\" folder\n",
    "output_folder = 'lm'\n",
    "model_name = 'Wikipedia_combined_Dev_Train_Fast'\n",
    "save_model(classifier, output_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619fadca-bde4-4885-b5b8-1967ee5854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion for final baseline model\n",
    "\n",
    "def train_classifier(word_features_file1, word_features_file2, word_features_file3, word_features_file4, word_features_file5, word_features_file6):\n",
    "    # Load the word_features DataFrames from the pickle files\n",
    "    with open(word_features_file1, 'rb') as file:\n",
    "        word_features1 = pickle.load(file)\n",
    "    with open(word_features_file2, 'rb') as file:\n",
    "        word_features2 = pickle.load(file)\n",
    "    with open(word_features_file3, 'rb') as file:\n",
    "        word_features3 = pickle.load(file)\n",
    "    with open(word_features_file4, 'rb') as file:\n",
    "        word_features4 = pickle.load(file)\n",
    "    with open(word_features_file5, 'rb') as file:\n",
    "        word_features5 = pickle.load(file)\n",
    "    with open(word_features_file6, 'rb') as file:\n",
    "        word_features6 = pickle.load(file)\n",
    "\n",
    "    # Concatenate the word_features DataFrames\n",
    "    word_features = pd.concat([word_features1, word_features2, word_features3, word_features4, word_features5, word_features6], ignore_index=True)\n",
    "\n",
    "    # Extract the features and labels from the combined word_features DataFrame\n",
    "    features_columns = ['syllables', 'length', 'vowels', 'simple_wiki_freq', 'HIT_freq', 'google frequency']\n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "   # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(model, output_folder, model_name):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save the model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{model_name}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009f7a80-44e5-4643-9fa2-0ef5a8458ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier and save the model\n",
    "word_features_file1 = 'features/WikiNews_Train_allInfo_1.pkl'\n",
    "word_features_file2 = 'features/WikiNews_Dev_allInfo_1.pkl'\n",
    "word_features_file3 = 'features/News_Train_allInfo_1.pkl'\n",
    "word_features_file4 = 'features/News_Dev_allInfo_1.pkl'\n",
    "word_features_file5 = 'features/WikiNews_Train_allInfo_1.pkl'\n",
    "word_features_file6 = 'features/WikiNews_Dev_allInfo_1.pkl'\n",
    "model = train_classifier(word_features_file1, word_features_file2, word_features_file3, word_features_file4, word_features_file5, word_features_file6)\n",
    "output_folder = 'lm'\n",
    "model_name = 'Baseline_Binary_allInfo'\n",
    "save_model(model, output_folder, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88601bbe-c80e-489e-bb2b-abf96fc0e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Build model with all CAMB features\n",
    "\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import os\n",
    "# from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "# def train_classifier(word_features_file1, word_features_file2, word_features_file3, word_features_file4, word_features_file5, word_features_file6):\n",
    "#     # Load the word_features DataFrames from the pickle files\n",
    "#     with open(word_features_file1, 'rb') as file:\n",
    "#         word_features1 = pickle.load(file)\n",
    "#     with open(word_features_file2, 'rb') as file:\n",
    "#         word_features2 = pickle.load(file)\n",
    "#     with open(word_features_file3, 'rb') as file:\n",
    "#         word_features3 = pickle.load(file)\n",
    "#     with open(word_features_file4, 'rb') as file:\n",
    "#         word_features4 = pickle.load(file)\n",
    "#     with open(word_features_file5, 'rb') as file:\n",
    "#         word_features5 = pickle.load(file)\n",
    "#     with open(word_features_file6, 'rb') as file:\n",
    "#         word_features6 = pickle.load(file)\n",
    "\n",
    "#     # Concatenate the word_features DataFrames\n",
    "#     word_features = pd.concat([word_features1, word_features2, word_features3, word_features4, word_features5, word_features6], ignore_index=True)\n",
    "\n",
    "#     # Extract the features and labels from the combined word_features DataFrame\n",
    "#     features_columns = ['syllables', 'length', 'vowels', 'pos', 'dep num', 'lemma', 'synonyms', 'hypernyms', 'hyponyms', 'wikipedia_freq', 'subtitles_freq', 'learner_corpus_freq', 'complex_lexicon', 'bnc_freq', 'ogden', 'simple_wiki', 'cald', 'sub_imdb', 'cnc', 'img', 'aoa', 'fam', 'google frequency', 'KFCAT', 'KFSMP', 'KFFRQ', 'NPHN', 'TLFRQ', 'holonyms', 'meronyms', 'consonants', 'learners_bigrams', 'simple_wiki_bigrams', 'ner', 'google_char_bigram', 'google_char_trigram', 'simple_wiki_fourgram', 'learner_fourgram', 'simple_wiki_freq', 'HIT_freq']\n",
    "#     X = word_features[features_columns].values\n",
    "#     y = word_features['complex_binary'].values\n",
    "\n",
    "#     # Create and train the Nearest Centroid classifier\n",
    "#     clf = NearestCentroid()\n",
    "#     clf.fit(X, y)\n",
    "\n",
    "#     return clf\n",
    "\n",
    "# def save_model(model, output_folder, model_name):\n",
    "#     # Create the output folder if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     # Save the model to a file\n",
    "#     output_file_path = os.path.join(output_folder, f'{model_name}_model.pkl')\n",
    "#     with open(output_file_path, 'wb') as file:\n",
    "#         pickle.dump(model, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af880c-54aa-4e34-86cc-b34fae5ecbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
